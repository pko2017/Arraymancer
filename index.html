<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--  This file is generated by Nim. -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Favicon -->
<link rel="shortcut icon" href="data:image/x-icon;base64,AAABAAEAEBAAAAEAIABoBAAAFgAAACgAAAAQAAAAIAAAAAEAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAUAAAAF////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAIAAABbAAAAlQAAAKIAAACbAAAAmwAAAKIAAACVAAAAWwAAAAL///8A////AP///wD///8A////AAAAABQAAADAAAAAYwAAAA3///8A////AP///wD///8AAAAADQAAAGMAAADAAAAAFP///wD///8A////AP///wAAAACdAAAAOv///wD///8A////AP///wD///8A////AP///wD///8AAAAAOgAAAJ3///8A////AP///wAAAAAnAAAAcP///wAAAAAoAAAASv///wD///8A////AP///wAAAABKAAAAKP///wAAAABwAAAAJ////wD///8AAAAAgQAAABwAAACIAAAAkAAAAJMAAACtAAAAFQAAABUAAACtAAAAkwAAAJAAAACIAAAAHAAAAIH///8A////AAAAAKQAAACrAAAAaP///wD///8AAAAARQAAANIAAADSAAAARf///wD///8AAAAAaAAAAKsAAACk////AAAAADMAAACcAAAAnQAAABj///8A////AP///wAAAAAYAAAAGP///wD///8A////AAAAABgAAACdAAAAnAAAADMAAAB1AAAAwwAAAP8AAADpAAAAsQAAAE4AAAAb////AP///wAAAAAbAAAATgAAALEAAADpAAAA/wAAAMMAAAB1AAAAtwAAAOkAAAD/AAAA/wAAAP8AAADvAAAA3gAAAN4AAADeAAAA3gAAAO8AAAD/AAAA/wAAAP8AAADpAAAAtwAAAGUAAAA/AAAA3wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAADfAAAAPwAAAGX///8A////AAAAAEgAAADtAAAAvwAAAL0AAADGAAAA7wAAAO8AAADGAAAAvQAAAL8AAADtAAAASP///wD///8A////AP///wD///8AAAAAO////wD///8A////AAAAAIcAAACH////AP///wD///8AAAAAO////wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A//8AAP//AAD4HwAA7/cAAN/7AAD//wAAoYUAAJ55AACf+QAAh+EAAAAAAADAAwAA4AcAAP5/AAD//wAA//8AAA=="/>
<link rel="icon" type="image/png" sizes="32x32" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAB3RJTUUH4QQQEwksSS9ZWwAAAk1JREFUWMPtll2ITVEUx39nn/O7Y5qR8f05wtCUUr6ZIS++8pEnkZInPImneaCQ5METNdOkeFBKUhMPRIkHKfEuUZSUlGlKPN2TrgfncpvmnntnmlEyq1Z7t89/rf9a6+y99oZxGZf/XeIq61EdtgKXgdXA0xrYAvBjOIF1AI9zvjcC74BSpndrJPkBWDScTF8Aa4E3wDlgHbASaANmVqlcCnwHvgDvgVfAJ+AikAAvgfVZwLnSVZHZaOuKoQi3ZOMi4NkYkpe1p4J7A8BpYAD49hfIy/oqG0+hLomiKP2L5L+1ubn5115S+3OAn4EnwBlgMzCjyt6ZAnQCJ4A7wOs88iRJHvw50HoujuPBoCKwHWiosy8MdfZnAdcHk8dxXFJ3VQbQlCTJvRBCGdRbD4M6uc5glpY3eAihpN5S5w12diSEcCCEcKUO4ljdr15T76ur1FDDLIQQ3qv71EdDOe3Kxj3leRXyk+pxdWnFWod6Wt2bY3de3aSuUHcPBVimHs7mK9WrmeOF6lR1o9qnzskh2ar2qm1qizpfXaPeVGdlmGN5pb09qMxz1Xb1kLqgzn1RyH7JUXW52lr5e/Kqi9qpto7V1atuUzfnARrV7jEib1T76gG2qxdGmXyiekkt1GswPTtek0aBfJp6YySGBfWg2tPQ0FAYgf1stUfdmdcjarbYJEniKIq6gY/Aw+zWHAC+p2labGpqiorFYgGYCEzN7oQdQClN07O1/EfDyGgC0ALMBdYAi4FyK+4H3gLPsxfR1zRNi+NP7nH5J+QntnXe5B5mpfQAAAAASUVORK5CYII=">

<!-- Google fonts -->
<link href='https://fonts.googleapis.com/css?family=Lato:400,600,900' rel='stylesheet' type='text/css'/>
<link href='https://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>

<!-- CSS -->
<title>Arraymancer</title>
<link rel="stylesheet" type="text/css" href="nimdoc.out.css">

<script type="text/javascript" src="dochack.js"></script>

<script type="text/javascript">
function main() {
  var pragmaDots = document.getElementsByClassName("pragmadots");
  for (var i = 0; i < pragmaDots.length; i++) {
    pragmaDots[i].onclick = function(event) {
      // Hide tease
      event.target.parentNode.style.display = "none";
      // Show actual
      event.target.parentNode.nextElementSibling.style.display = "inline";
    }
  }

  const toggleSwitch = document.querySelector('.theme-switch input[type="checkbox"]');
  function switchTheme(e) {
      if (e.target.checked) {
          document.documentElement.setAttribute('data-theme', 'dark');
          localStorage.setItem('theme', 'dark');
      } else {
          document.documentElement.setAttribute('data-theme', 'light');
          localStorage.setItem('theme', 'light');
      }
  }

  toggleSwitch.addEventListener('change', switchTheme, false);


  if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
    document.documentElement.setAttribute('data-theme', "dark");
    toggleSwitch.checked = true;
  } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: light)').matches) {
    document.documentElement.setAttribute('data-theme', "light");
    toggleSwitch.checked = false;
  } else {
    const currentTheme = localStorage.getItem('theme') ? localStorage.getItem('theme') : null;
    if (currentTheme) {
      document.documentElement.setAttribute('data-theme', currentTheme);

      if (currentTheme === 'dark') {
        toggleSwitch.checked = true;
      }
    }
  }
}
</script>

</head>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Arraymancer - Arraymancer</title>

<link href="docutils.css" rel="stylesheet" type="text/css"/>
<link href="nav.css" rel="stylesheet" type="text/css"/>

<link href='http://fonts.googleapis.com/css?family=Raleway:400,600,900' rel='stylesheet' type='text/css'/>
<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>

<a href="https://github.com/mratsim/arraymancer"><img style="position: fixed; top: 0; right: 0; border: 0; z-index: 10;" src="https://camo.githubusercontent.com/652c5b9acfaddf3a9c326fa6bde407b87f7be0f4/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6f72616e67655f6666373630302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>

<body onload="main()">
<div class="document" id="documentId">
  <div class="container">
    <h1 class="title">Arraymancer</h1>
    
<h1 id="arraymancer-minus-a-nminusdimensional-tensor-ndarray-librarydot">Arraymancer - A n-dimensional tensor (ndarray) library.</h1><p>Arraymancer is a tensor (N-dimensional array) project in Nim. The main focus is providing a fast and ergonomic CPU and GPU ndarray library on which to build a scientific computing and in particular a deep learning ecosystem.</p>
<p>The library is inspired by Numpy and PyTorch. The library provides ergonomics very similar to Numpy, Julia and Matlab but is fully parallel and significantly faster than those libraries. It is also faster than C-based Torch.</p>
<p>Note: While Nim is compiled and does not offer an interactive REPL yet (like Jupyter), it allows much faster prototyping than C++ due to extremely fast compilation times. Arraymancer compiles in about 5 seconds on my dual-core MacBook.</p>

<h2 id="why-arraymancer">Why Arraymancer</h2>
<h3 id="the-python-community-is-struggling-to-bring-numpy-upminustominusspeed">The Python community is struggling to bring Numpy up-to-speed</h3><ul class="simple"><li>Numba JIT compiler</li>
<li>Dask delayed parallel computation graph</li>
<li>Cython to ease numerical computations in Python</li>
<li>Due to the GIL shared-memory parallelism (OpenMP) is not possible in pure Python</li>
<li>Use “vectorized operations” (i.e. don’t use for loops in Python)</li>
</ul>
<p>Why not use in a single language with all the blocks to build the most efficient scientific computing library with Python ergonomics.</p>
<p>OpenMP batteries included.</p>

<h3 id="a-researcher-workflow-is-a-fight-against-inefficiencies">A researcher workflow is a fight against inefficiencies</h3><p>Researchers in a heavy scientific computing domain often have the following workflow: Mathematica/Matlab/Python/R (prototyping) -&gt; C/C++/Fortran (speed, memory)</p>
<p>Why not use in a language as productive as Python and as fast as C? Code once, and don’t spend months redoing the same thing at a lower level.</p>

<h3 id="tools-available-in-labs-are-not-available-in-productioncolon">Tools available in labs are not available in production:</h3><ul class="simple"><li>Managing and deploying Python (2.7, 3.5, 3.6) and packages version in a robust manner requires devops-fu (virtualenv, Docker, …)</li>
<li>Python data science ecosystem does not run on embedded devices (Nvidia Tegra/drones) or mobile phones, especially preprocessing dependencies.</li>
</ul>
<p>Nim is compiled, no need to worry about version conflicts and the whole toolchain being in need of fixing due to Python version updates. Furthermore, as long as your platform supports C, Arraymancer will run on it from Raspberry Pi to mobile phones and drones.</p>
<p>Note: Arraymancer Cuda/Cudnn backend is shaping up and convolutional neural nets are just around the corner.</p>

<h3 id="bridging-the-gap-between-deep-learning-research-and-production">Bridging the gap between deep learning research and production</h3><p>The deep learning frameworks are currently in two camps: - Research: Theano, Tensorflow, Keras, Torch, PyTorch - Production: Caffe, Darknet, (Tensorflow)</p>
<p>Furthermore, Python preprocessing steps, unless using OpenCV, often needs a custom implementation (think text/speech preprocessing on phones).</p>
<ul class="simple"><li>Tensorflow is supposed to bridge the gap between research and production but its syntax and ergonomics are a pain to work with. It’s the same issue as researchers, “Prototype in Keras, and when you need low-level –&gt; Tensorflow”.</li>
<li>Deployed models are static, there is no interface to add a new observation/training sample to any framework, the end goal being to use a model as a webservice.</li>
</ul>

<h3 id="so-why-arraymancer-qmark">So why Arraymancer ?</h3><p>All those pain points may seem like a huge undertaking however thanks to the Nim language, we can have Arraymancer: - Be as fast as C - Accelerated routines with Intel MKL/OpenBLAS or even NNPACK - Access to CUDA and CuDNN and generate custom CUDA kernels on the fly via metaprogramming.</p>
<blockquote><p><ul class="simple"><li>A Python-like syntax with custom operators <tt class="docutils literal"><span class="pre">a * b</span></tt></li>
</ul>
<blockquote><p>for tensor multiplication instead of <tt class="docutils literal"><span class="pre">a.dot(b)</span></tt> (Numpy/Tensorflow) or <tt class="docutils literal"><span class="pre">a.mm(b)</span></tt> (Torch) - Numpy-like slicing ergonomics <tt class="docutils literal"><span class="pre">t[0..4, 2..10|2]</span></tt></p></blockquote>
<ul class="simple"><li>For everything that Nim doesn’t have yet, you can use Nim bindings to</li>
</ul>
<blockquote><p>C, C++, Objective-C or Javascript to bring it to Nim. Nim also has unofficial Python-&gt;Nim and Nim-&gt;Python wrappers.</p></blockquote>
</p></blockquote>

<h2 id="future-ambitions">Future ambitions</h2><p>Because apparently to be successful you need a vision, I would like Arraymancer to be:</p>
<ul class="simple"><li>The go-to tool for Deep Learning video processing. I.e. <tt class="docutils literal"><span class="pre">vid = load_video(&quot;./cats/youtube_cat_video.mkv&quot;)</span></tt></li>
<li>Target javascript, WebAssembly, Apple Metal, ARM devices, AMD Rocm, OpenCL, you name it.</li>
<li>Target cryptominers FPGAs because they drove the price of GPUs for honest deep-learners too high.</li>
<li>The base of a Starcraft II AI bot.</li>
</ul>

<h2 id="support-types-os-hardware">Support (Types, OS, Hardware)</h2><p>Arraymancer’s tensors supports arbitrary types (floats, strings, objects …).</p>
<p>Arraymancer run anywhere you can compile C code. Linux, MacOS are supported, Windows should work too as Appveyor (Continuous Integration for Windows) never flash red. Optionally you can compile Arraymancer with Cuda support.</p>
<p>Note: Arraymancer Tensors and CudaTensors are tensors in the machine learning sense (multidimensional array) not in the mathematical sense (describe transformation laws)</p>

<h2 id="limitationscolon">Limitations:</h2><p>EXPERIMENTAL: Arraymancer may summon Ragnarok and cause the heat death of the Universe.</p>
<ol class="simple"><li>Display of 5-dimensional or more tensors is not implemented. (To be honest Christopher Nolan had the same issue in Interstellar)</li>
</ol>

<h2 id="installationcolon">Installation:</h2><p>Nim is available in some Linux repositories and on Homebrew for macOS.</p>
<p>I however recommend installing Nim in your user profile via <a class="reference external" href="https://github.com/dom96/choosenim">choosenim</a>. Once choosenim installed Nim, you can <tt class="docutils literal"><span class="pre">nimble install arraymancer</span></tt> which will pull arraymancer and all its dependencies.</p>

<h3 id="tensors-on-cpu-and-on-cuda">Tensors on CPU and on Cuda</h3><p>Tensors and CudaTensors do not have the same features implemented yet. Also Cuda Tensors can only be float32 or float64 while Cpu Tensor can be integers, string, boolean or any custom object.</p>
<p>Here is a comparative table.</p>
<table border="1" class="docutils"><tr><th>Feature</th><th>Tensor</th><th>CudaTensor</th><th>ClTensor</th></tr>
<tr><td>Accessing tensor properties</td><td>[x]</td><td>[x]</td><td>[x]</td></tr>
<tr><td>Tensor creation</td><td>[x]</td><td>by converting a cpu Tensor</td><td>by converting a cpu Tensor</td></tr>
<tr><td>Accessing or modifying a single value</td><td>[x]</td><td>[]</td><td>[]</td></tr>
<tr><td>Iterating on a Tensor</td><td>[x]</td><td>[]</td><td>[]</td></tr>
<tr><td>Slicing a Tensor</td><td>[x]</td><td>[x]</td><td>[x]</td></tr>
<tr><td>Slice mutation <tt class="docutils literal"><span class="pre">a[1,_] = 10</span></tt></td><td>[x]</td><td>[]</td><td>[]</td></tr>
<tr><td>Comparison <tt class="docutils literal"><span class="pre">==</span></tt></td><td>[x]</td><td>[]</td><td>[]</td></tr>
<tr><td>Element-wise basic operations</td><td>[x]</td><td>[x]</td><td>[x]</td></tr>
<tr><td>Universal functions</td><td>[x]</td><td>[]</td><td>[]</td></tr>
<tr><td>Automatically broadcasted operations</td><td>[x]</td><td>[x]</td><td>[x]</td></tr>
<tr><td>Matrix-Matrix and Matrix vector multiplication</td><td>[x]</td><td>[x]</td><td>[x]</td></tr>
<tr><td>Displaying a tensor</td><td>[x]</td><td>[x]</td><td>[x]</td></tr>
<tr><td>Higher-order functions (map, apply, reduce, fold)</td><td>[x]</td><td>internal only</td><td>internal only</td></tr>
<tr><td>Transposing</td><td>[x]</td><td>[x]</td><td>[]</td></tr>
<tr><td>Converting to contiguous</td><td>[x]</td><td>[x]</td><td>[]</td></tr>
<tr><td>Reshaping</td><td>[x]</td><td>[x]</td><td>[]</td></tr>
<tr><td>Explicit broadcast</td><td>[x]</td><td>[x]</td><td>[x]</td></tr>
<tr><td>Permuting dimensions</td><td>[x]</td><td>[]</td><td>[]</td></tr>
<tr><td>Concatenating along existing dimensions</td><td>[x]</td><td>[]</td><td>[]</td></tr>
<tr><td>Squeezing singleton dimensions</td><td>[x]</td><td>[x]</td><td>[]</td></tr>
<tr><td>Slicing + squeezing in one operation</td><td>[x]</td><td>[]</td><td>[]</td></tr>
</table>


    <div class="row">
      <div class="twelve-columns footer">
        <span class="nim-sprite"></span>
        <br/>
        <small style="color: var(--hint);">Made with Nim. Generated: 2020-04-19 13:42:51 UTC</small>
      </div>
    </div>
  </div>
</div>

<header>
  <a class="pagetitle" href="index.html">Arraymancer</a>
  <span>
    <a href="#">Technical reference</a>
    <ul class="monospace">
      <span>
        <a href="#">Core tensor API</a>
        <ul class="monospace">
          <li><a href="accessors.html">accessors</a></li>
<li><a href="accessors_macros_read.html">accessors_macros_read</a></li>
<li><a href="accessors_macros_syntax.html">accessors_macros_syntax</a></li>
<li><a href="accessors_macros_write.html">accessors_macros_write</a></li>
<li><a href="aggregate.html">aggregate</a></li>
<li><a href="blas_l3_gemm.html">blas_l3_gemm</a></li>
<li><a href="cublas.html">cublas</a></li>
<li><a href="cuda.html">cuda</a></li>
<li><a href="cuda_global_state.html">cuda_global_state</a></li>
<li><a href="data_structure.html">data_structure</a></li>
<li><a href="display.html">display</a></li>
<li><a href="display_cuda.html">display_cuda</a></li>
<li><a href="einsum.html">einsum</a></li>
<li><a href="exporting.html">exporting</a></li>
<li><a href="filling_data.html">filling_data</a></li>
<li><a href="higher_order_applymap.html">higher_order_applymap</a></li>
<li><a href="higher_order_foldreduce.html">higher_order_foldreduce</a></li>
<li><a href="incl_accessors_cuda.html">incl_accessors_cuda</a></li>
<li><a href="incl_higher_order_cuda.html">incl_higher_order_cuda</a></li>
<li><a href="incl_kernels_cuda.html">incl_kernels_cuda</a></li>
<li><a href="init_copy_cpu.html">init_copy_cpu</a></li>
<li><a href="init_copy_cuda.html">init_copy_cuda</a></li>
<li><a href="init_cpu.html">init_cpu</a></li>
<li><a href="init_cuda.html">init_cuda</a></li>
<li><a href="init_opencl.html">init_opencl</a></li>
<li><a href="lapack.html">lapack</a></li>
<li><a href="math_functions.html">math_functions</a></li>
<li><a href="memory_optimization_hints.html">memory_optimization_hints</a></li>
<li><a href="metadataArray.html">metadataArray</a></li>
<li><a href="naive_l2_gemv.html">naive_l2_gemv</a></li>
<li><a href="opencl_backend.html">opencl_backend</a></li>
<li><a href="opencl_global_state.html">opencl_global_state</a></li>
<li><a href="openmp.html">openmp</a></li>
<li><a href="operators_blas_l1.html">operators_blas_l1</a></li>
<li><a href="operators_blas_l1_cuda.html">operators_blas_l1_cuda</a></li>
<li><a href="operators_blas_l1_opencl.html">operators_blas_l1_opencl</a></li>
<li><a href="operators_blas_l2l3.html">operators_blas_l2l3</a></li>
<li><a href="operators_blas_l2l3_cuda.html">operators_blas_l2l3_cuda</a></li>
<li><a href="operators_blas_l2l3_opencl.html">operators_blas_l2l3_opencl</a></li>
<li><a href="operators_broadcasted.html">operators_broadcasted</a></li>
<li><a href="operators_broadcasted_cuda.html">operators_broadcasted_cuda</a></li>
<li><a href="operators_broadcasted_opencl.html">operators_broadcasted_opencl</a></li>
<li><a href="operators_comparison.html">operators_comparison</a></li>
<li><a href="operators_logical.html">operators_logical</a></li>
<li><a href="optim_ops_fusion.html">optim_ops_fusion</a></li>
<li><a href="p_accessors.html">p_accessors</a></li>
<li><a href="p_accessors_macros_desugar.html">p_accessors_macros_desugar</a></li>
<li><a href="p_accessors_macros_read.html">p_accessors_macros_read</a></li>
<li><a href="p_accessors_macros_write.html">p_accessors_macros_write</a></li>
<li><a href="p_checks.html">p_checks</a></li>
<li><a href="p_complex.html">p_complex</a></li>
<li><a href="p_display.html">p_display</a></li>
<li><a href="p_init_cpu.html">p_init_cpu</a></li>
<li><a href="p_init_cuda.html">p_init_cuda</a></li>
<li><a href="p_init_opencl.html">p_init_opencl</a></li>
<li><a href="p_kernels_interface_cuda.html">p_kernels_interface_cuda</a></li>
<li><a href="p_kernels_interface_opencl.html">p_kernels_interface_opencl</a></li>
<li><a href="p_operator_blas_l2l3.html">p_operator_blas_l2l3</a></li>
<li><a href="p_shapeshifting.html">p_shapeshifting</a></li>
<li><a href="selectors.html">selectors</a></li>
<li><a href="shapeshifting.html">shapeshifting</a></li>
<li><a href="shapeshifting_cuda.html">shapeshifting_cuda</a></li>
<li><a href="shapeshifting_opencl.html">shapeshifting_opencl</a></li>
<li><a href="syntactic_sugar.html">syntactic_sugar</a></li>
<li><a href="tensor.html">tensor</a></li>
<li><a href="tensor_cuda.html">tensor_cuda</a></li>
<li><a href="tensor_opencl.html">tensor_opencl</a></li>
<li><a href="ufunc.html">ufunc</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Neural network API</a>
        <ul class="monospace">
          <li><a href="conv2D.html">Layers: Convolution 2D</a></li>
<li><a href="cross_entropy_losses.html">Loss: Cross-Entropy losses</a></li>
<li><a href="dsl_core.html">Neural network: Declaration</a></li>
<li><a href="dsl_forwardsugar.html">dsl_forwardsugar</a></li>
<li><a href="dsl_initialization.html">dsl_initialization</a></li>
<li><a href="dsl_topology.html">dsl_topology</a></li>
<li><a href="dsl_types.html">dsl_types</a></li>
<li><a href="dsl_utils.html">dsl_utils</a></li>
<li><a href="embedding.html">Layers: Embedding</a></li>
<li><a href="gru.html">Layers: GRU (Gated Linear Unit)</a></li>
<li><a href="init.html">Layers: Initializations</a></li>
<li><a href="linear.html">Layers: Linear/Dense</a></li>
<li><a href="maxpool2D.html">Layers: Maxpool 2D</a></li>
<li><a href="mean_square_error_loss.html">Loss: Mean Square Error</a></li>
<li><a href="optimizers.html">Optimizers</a></li>
<li><a href="relu.html">Activation: Relu (Rectified linear Unit)</a></li>
<li><a href="sigmoid.html">Activation: Sigmoid</a></li>
<li><a href="softmax.html">Softmax</a></li>
<li><a href="tanh.html">Activation: Tanh</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Linear algebra, stats, ML</a>
        <ul class="monospace">
          <li><a href="accuracy_score.html">Accuracy score</a></li>
<li><a href="auxiliary_blas.html">auxiliary_blas</a></li>
<li><a href="auxiliary_lapack.html">auxiliary_lapack</a></li>
<li><a href="common_error_functions.html">Common errors, MAE and MSE (L1, L2 loss)</a></li>
<li><a href="decomposition.html">Eigenvalue decomposition</a></li>
<li><a href="decomposition_lapack.html">decomposition_lapack</a></li>
<li><a href="decomposition_rand.html">Randomized Truncated SVD</a></li>
<li><a href="init_colmajor.html">init_colmajor</a></li>
<li><a href="kmeans.html">K-Means</a></li>
<li><a href="least_squares.html">Least squares solver</a></li>
<li><a href="least_squares_lapack.html">least_squares_lapack</a></li>
<li><a href="linear_algebra.html">linear_algebra</a></li>
<li><a href="linear_systems.html">Linear systems solver</a></li>
<li><a href="overload.html">overload</a></li>
<li><a href="pca.html">Principal Component Analysis (PCA)</a></li>
<li><a href="solve_lapack.html">solve_lapack</a></li>
<li><a href="special_matrices.html">Special linear algebra matrices</a></li>
<li><a href="stats.html">Statistics</a></li>
<li><a href="triangular.html">triangular</a></li>
        </ul>
      </span>
      <span>
        <a href="#">IO & Datasets</a>
        <ul class="monospace">
          <li><a href="imdb.html">IMDB</a></li>
<li><a href="io_csv.html">CSV reading and writing</a></li>
<li><a href="io_hdf5.html">HDF5 files reading and writing</a></li>
<li><a href="io_image.html">Images reading and writing</a></li>
<li><a href="io_npy.html">Numpy files reading and writing</a></li>
<li><a href="io_stream_readers.html">io_stream_readers</a></li>
<li><a href="mnist.html">MNIST</a></li>
<li><a href="util.html">util</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Autograd</a>
        <ul class="monospace">
          <li><a href="autograd_common.html">Data structure</a></li>
<li><a href="gates_basic.html">Basic operations</a></li>
<li><a href="gates_blas.html">Linear algebra operations</a></li>
<li><a href="gates_hadamard.html">Hadamard product (elementwise matrix multiply)</a></li>
<li><a href="gates_reduce.html">Reduction operations</a></li>
<li><a href="gates_shapeshifting_concat_split.html">Concatenation, stacking, splitting, chunking operations</a></li>
<li><a href="gates_shapeshifting_views.html">Linear algebra operations</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Neuralnet primitives</a>
        <ul class="monospace">
          <li><a href="conv.html">conv</a></li>
<li><a href="cudnn.html">cudnn</a></li>
<li><a href="cudnn_conv_interface.html">cudnn_conv_interface</a></li>
<li><a href="nn_primitives.html">nn_primitives</a></li>
<li><a href="nnp_activation.html">Activations</a></li>
<li><a href="nnp_conv2d_cudnn.html">Convolution 2D - CuDNN</a></li>
<li><a href="nnp_convolution.html">Convolution 2D</a></li>
<li><a href="nnp_embedding.html">Embeddings</a></li>
<li><a href="nnp_gru.html">Gated Recurrent Unit (GRU)</a></li>
<li><a href="nnp_linear.html">Linear / Dense layer</a></li>
<li><a href="nnp_maxpooling.html">Maxpooling</a></li>
<li><a href="nnp_numerical_gradient.html">Numerical gradient</a></li>
<li><a href="nnp_sigmoid_cross_entropy.html">Sigmoid Cross-Entropy loss</a></li>
<li><a href="nnp_softmax.html">Softmax</a></li>
<li><a href="nnp_softmax_cross_entropy.html">Softmax Cross-Entropy loss</a></li>
<li><a href="nnpack.html">nnpack</a></li>
<li><a href="nnpack_interface.html">nnpack_interface</a></li>
<li><a href="p_activation.html">p_activation</a></li>
<li><a href="p_logsumexp.html">p_logsumexp</a></li>
<li><a href="p_nnp_checks.html">p_nnp_checks</a></li>
<li><a href="p_nnp_types.html">p_nnp_types</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Other docs</a>
        <ul class="monospace">
          <li><a href="align_unroller.html">align_unroller</a></li>
<li><a href="ast_utils.html">ast_utils</a></li>
<li><a href="compiler_optim_hints.html">compiler_optim_hints</a></li>
<li><a href="cpuinfo_x86.html">cpuinfo_x86</a></li>
<li><a href="functional.html">functional</a></li>
<li><a href="gemm.html">gemm</a></li>
<li><a href="gemm_packing.html">gemm_packing</a></li>
<li><a href="gemm_prepacked.html">gemm_prepacked</a></li>
<li><a href="gemm_tiling.html">gemm_tiling</a></li>
<li><a href="gemm_ukernel_avx.html">gemm_ukernel_avx</a></li>
<li><a href="gemm_ukernel_avx2.html">gemm_ukernel_avx2</a></li>
<li><a href="gemm_ukernel_avx512.html">gemm_ukernel_avx512</a></li>
<li><a href="gemm_ukernel_avx_fma.html">gemm_ukernel_avx_fma</a></li>
<li><a href="gemm_ukernel_dispatch.html">gemm_ukernel_dispatch</a></li>
<li><a href="gemm_ukernel_generator.html">gemm_ukernel_generator</a></li>
<li><a href="gemm_ukernel_generic.html">gemm_ukernel_generic</a></li>
<li><a href="gemm_ukernel_sse.html">gemm_ukernel_sse</a></li>
<li><a href="gemm_ukernel_sse2.html">gemm_ukernel_sse2</a></li>
<li><a href="gemm_ukernel_sse4_1.html">gemm_ukernel_sse4_1</a></li>
<li><a href="gemm_utils.html">gemm_utils</a></li>
<li><a href="global_config.html">global_config</a></li>
<li><a href="math_ops_fusion.html">math_ops_fusion</a></li>
<li><a href="memory.html">memory</a></li>
<li><a href="nested_containers.html">nested_containers</a></li>
<li><a href="nlp.html">nlp</a></li>
<li><a href="openmp.html">openmp</a></li>
<li><a href="sequninit.html">sequninit</a></li>
<li><a href="simd.html">simd</a></li>
<li><a href="tokenizers.html">tokenizers</a></li>
        </ul>
      </span>
    </ul>
  </span>
  <span>
    <a href="#">Tutorial</a>
    <ul class="monospace">
      <li><a href="tuto.first_steps.html">First steps</a></li>
      <li><a href="tuto.slicing.html">Taking a slice of a tensor</a></li>
      <li><a href="tuto.linear_algebra.html">Matrix & vectors operations</a></li>
      <li><a href="tuto.broadcasting.html">Broadcasted operations</a></li>
      <li><a href="tuto.shapeshifting.html">Transposing, Reshaping, Permuting, Concatenating</a></li>
      <li><a href="tuto.map_reduce.html">Map & Reduce</a></li>
      <li><a href="tuto.iterators.html">Basic iterators</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Spellbook (How-To&apos;s)</a>
    <ul class="monospace">
      <li><a href="howto.type_conversion.html">How to convert a Tensor type?</a></li>
      <li><a href="howto.ufunc.html">How to create a new universal function?</a></li>
      <li><a href="howto.perceptron.html">How to create a multilayer perceptron?</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Under the hood</a>
    <ul class="monospace">
      <li><a href="uth.speed.html">How Arraymancer achieves its speed?</a></li>
      <li><a href="uth.copy_semantics.html">Why does `=` share data by default aka reference semantics?</a></li>
      <li><a href="uth.opencl_cuda_nim.html">Working with OpenCL and Cuda in Nim</a></li>
    </ul>
  </span>
</header>
</body>
</html>
